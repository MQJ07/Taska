{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f1cc4d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "9f4d00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"E:/Internshp/dataset.txt\", \"r\", encoding = \"utf8\")\n",
    "lines = []\n",
    "\n",
    "for i in file:\n",
    "    lines.append(i)\n",
    "    \n",
    "data = \"\"\n",
    "\n",
    "for i in lines:\n",
    "    data = ' '. join(lines)\n",
    "    \n",
    "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '')\n",
    "\n",
    "translator = str.maketrans(string.punctuation, ' '*len(string.punctuation)) #map punctuation to space\n",
    "new_data = data.translate(translator)\n",
    "\n",
    "z = []\n",
    "\n",
    "for i in data.split():\n",
    "    if i not in z:\n",
    "        z.append(i)\n",
    "        \n",
    "data = ' '.join(z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "61a80e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([data])\n",
    "\n",
    "# saving the tokenizer for predict function.\n",
    "pickle.dump(tokenizer, open('tokenizer1.pkl', 'wb'))\n",
    "\n",
    "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
    "\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for i in range(1, len(sequence_data)):\n",
    "    words = sequence_data[i-1:i+1]\n",
    "    sequences.append(words)\n",
    "    \n",
    "sequences = np.array(sequences)\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in sequences:\n",
    "    X.append(i[0])\n",
    "    y.append(i[1])\n",
    "    \n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "y = to_categorical(y, num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "69811da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aad147a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 10, input_length=1))\n",
    "model.add(LSTM(1000, return_sequences=True))\n",
    "model.add(LSTM(1000))\n",
    "model.add(Dense(1000, activation=\"relu\"))\n",
    "model.add(Dense(vocab_size, activation=\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8e9e49b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1, 10)             30540     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3054)              3057054   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,136,594\n",
      "Trainable params: 16,136,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a31b234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n",
    "    save_best_only=True, mode='auto')\n",
    "\n",
    "reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n",
    "\n",
    "logdir='logsnextword1'\n",
    "tensorboard_Visualization = TensorBoard(log_dir=logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5ff94026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0b341b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 8.0296\n",
      "Epoch 1: loss improved from inf to 8.02966, saving model to nextword1.h5\n",
      "74/74 [==============================] - 10s 76ms/step - loss: 8.0297 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 7.9996\n",
      "Epoch 2: loss improved from 8.02966 to 7.99956, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 7.9996 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 7.8946\n",
      "Epoch 3: loss improved from 7.99956 to 7.89459, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 7.8946 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 7.6697\n",
      "Epoch 4: loss improved from 7.89459 to 7.66974, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 7.6697 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 7.3349\n",
      "Epoch 5: loss improved from 7.66974 to 7.33488, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 7.3349 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 7.0020\n",
      "Epoch 6: loss improved from 7.33488 to 7.00205, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 7.0020 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 6.7172\n",
      "Epoch 7: loss improved from 7.00205 to 6.71724, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 6.7172 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 6.4795\n",
      "Epoch 8: loss improved from 6.71724 to 6.48068, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 6.4807 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 6.2598\n",
      "Epoch 9: loss improved from 6.48068 to 6.25979, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 6.2598 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 6.0656\n",
      "Epoch 10: loss improved from 6.25979 to 6.06664, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 76ms/step - loss: 6.0666 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 5.8738\n",
      "Epoch 11: loss improved from 6.06664 to 5.87383, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 5.8738 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 5.6988\n",
      "Epoch 12: loss improved from 5.87383 to 5.69644, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 5.6964 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 5.5585\n",
      "Epoch 13: loss improved from 5.69644 to 5.55853, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 5.5585 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 5.4193\n",
      "Epoch 14: loss improved from 5.55853 to 5.41932, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 5.4193 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 5.2891\n",
      "Epoch 15: loss improved from 5.41932 to 5.28911, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 5.2891 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 5.1927\n",
      "Epoch 16: loss improved from 5.28911 to 5.19275, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 5.1927 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 5.0824\n",
      "Epoch 17: loss improved from 5.19275 to 5.08243, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 5.0824 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.9824\n",
      "Epoch 18: loss improved from 5.08243 to 4.98242, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 4.9824 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.8932\n",
      "Epoch 19: loss improved from 4.98242 to 4.89317, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.8932 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.8284\n",
      "Epoch 20: loss improved from 4.89317 to 4.82842, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.8284 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 4.7608\n",
      "Epoch 21: loss improved from 4.82842 to 4.76038, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.7604 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.6656\n",
      "Epoch 22: loss improved from 4.76038 to 4.66563, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.6656 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.5956\n",
      "Epoch 23: loss improved from 4.66563 to 4.59564, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.5956 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.5002\n",
      "Epoch 24: loss improved from 4.59564 to 4.50023, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.5002 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.3820\n",
      "Epoch 25: loss improved from 4.50023 to 4.38195, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.3820 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.2734\n",
      "Epoch 26: loss improved from 4.38195 to 4.27339, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.2734 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.1291\n",
      "Epoch 27: loss improved from 4.27339 to 4.12907, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.1291 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 4.0395\n",
      "Epoch 28: loss improved from 4.12907 to 4.03947, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 4.0395 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.9161\n",
      "Epoch 29: loss improved from 4.03947 to 3.91608, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 3.9161 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.7773\n",
      "Epoch 30: loss improved from 3.91608 to 3.77732, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 3.7773 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.6602\n",
      "Epoch 31: loss improved from 3.77732 to 3.66018, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 3.6602 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.5471\n",
      "Epoch 32: loss improved from 3.66018 to 3.54715, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 3.5471 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.4187\n",
      "Epoch 33: loss improved from 3.54715 to 3.41866, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 3.4187 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.3232\n",
      "Epoch 34: loss improved from 3.41866 to 3.32320, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 3.3232 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.2423\n",
      "Epoch 35: loss improved from 3.32320 to 3.24226, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 3.2423 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.1380\n",
      "Epoch 36: loss improved from 3.24226 to 3.13804, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 3.1380 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.0571\n",
      "Epoch 37: loss improved from 3.13804 to 3.05713, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 3.0571 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 3.0007\n",
      "Epoch 38: loss improved from 3.05713 to 3.00068, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 3.0007 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.9205\n",
      "Epoch 39: loss improved from 3.00068 to 2.92054, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.9205 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.8600\n",
      "Epoch 40: loss improved from 2.92054 to 2.85999, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.8600 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.8220\n",
      "Epoch 41: loss improved from 2.85999 to 2.82196, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.8220 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.7489\n",
      "Epoch 42: loss improved from 2.82196 to 2.74891, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.7489 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.6608\n",
      "Epoch 43: loss improved from 2.74891 to 2.66081, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.6608 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.5963\n",
      "Epoch 44: loss improved from 2.66081 to 2.59633, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.5963 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.5516\n",
      "Epoch 45: loss improved from 2.59633 to 2.55157, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.5516 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.5061\n",
      "Epoch 46: loss improved from 2.55157 to 2.50609, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.5061 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.4595\n",
      "Epoch 47: loss improved from 2.50609 to 2.45952, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 2.4595 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.4123\n",
      "Epoch 48: loss improved from 2.45952 to 2.41235, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 2.4123 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.3505\n",
      "Epoch 49: loss improved from 2.41235 to 2.35051, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 2.3505 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.3042\n",
      "Epoch 50: loss improved from 2.35051 to 2.30416, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 2.3042 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.2513\n",
      "Epoch 51: loss improved from 2.30416 to 2.25127, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 2.2513 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.2142\n",
      "Epoch 52: loss improved from 2.25127 to 2.21425, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 2.2142 - lr: 0.0010\n",
      "Epoch 53/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.1678\n",
      "Epoch 53: loss improved from 2.21425 to 2.16781, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 2.1678 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.1173\n",
      "Epoch 54: loss improved from 2.16781 to 2.11733, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 2.1173 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.0941\n",
      "Epoch 55: loss improved from 2.11733 to 2.09406, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 2.0941 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.0379\n",
      "Epoch 56: loss improved from 2.09406 to 2.03789, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 2.0379 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 2.0338\n",
      "Epoch 57: loss improved from 2.03789 to 2.03376, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 2.0338 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.9712\n",
      "Epoch 58: loss improved from 2.03376 to 1.97118, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.9712 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.8986\n",
      "Epoch 59: loss improved from 1.97118 to 1.89863, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.8986 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.9102\n",
      "Epoch 60: loss did not improve from 1.89863\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 1.9102 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.8637\n",
      "Epoch 61: loss improved from 1.89863 to 1.86371, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.8637 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.8352\n",
      "Epoch 62: loss improved from 1.86371 to 1.83523, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.8352 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.7617\n",
      "Epoch 63: loss improved from 1.83523 to 1.76167, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 1.7617 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.7276\n",
      "Epoch 64: loss improved from 1.76167 to 1.72759, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.7276 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.6897\n",
      "Epoch 65: loss improved from 1.72759 to 1.68965, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.6897 - lr: 0.0010\n",
      "Epoch 66/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.6488\n",
      "Epoch 66: loss improved from 1.68965 to 1.64879, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 1.6488 - lr: 0.0010\n",
      "Epoch 67/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.6277\n",
      "Epoch 67: loss improved from 1.64879 to 1.62768, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.6277 - lr: 0.0010\n",
      "Epoch 68/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.5889\n",
      "Epoch 68: loss improved from 1.62768 to 1.58894, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.5889 - lr: 0.0010\n",
      "Epoch 69/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.5518\n",
      "Epoch 69: loss improved from 1.58894 to 1.55181, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.5518 - lr: 0.0010\n",
      "Epoch 70/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.5505\n",
      "Epoch 70: loss improved from 1.55181 to 1.55095, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 1.5510 - lr: 0.0010\n",
      "Epoch 71/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.5348\n",
      "Epoch 71: loss improved from 1.55095 to 1.53483, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 1.5348 - lr: 0.0010\n",
      "Epoch 72/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.5092\n",
      "Epoch 72: loss improved from 1.53483 to 1.50916, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 1.5092 - lr: 0.0010\n",
      "Epoch 73/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.4496\n",
      "Epoch 73: loss improved from 1.50916 to 1.45099, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.4510 - lr: 0.0010\n",
      "Epoch 74/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.4436\n",
      "Epoch 74: loss improved from 1.45099 to 1.44360, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.4436 - lr: 0.0010\n",
      "Epoch 75/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.3831\n",
      "Epoch 75: loss improved from 1.44360 to 1.38308, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.3831 - lr: 0.0010\n",
      "Epoch 76/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.3746\n",
      "Epoch 76: loss improved from 1.38308 to 1.37460, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.3746 - lr: 0.0010\n",
      "Epoch 77/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.3457\n",
      "Epoch 77: loss improved from 1.37460 to 1.34709, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.3471 - lr: 0.0010\n",
      "Epoch 78/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.3263\n",
      "Epoch 78: loss improved from 1.34709 to 1.32632, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.3263 - lr: 0.0010\n",
      "Epoch 79/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.3121\n",
      "Epoch 79: loss improved from 1.32632 to 1.31210, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.3121 - lr: 0.0010\n",
      "Epoch 80/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2935\n",
      "Epoch 80: loss improved from 1.31210 to 1.29354, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.2935 - lr: 0.0010\n",
      "Epoch 81/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2803\n",
      "Epoch 81: loss improved from 1.29354 to 1.28030, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.2803 - lr: 0.0010\n",
      "Epoch 82/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.2877\n",
      "Epoch 82: loss did not improve from 1.28030\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 1.2892 - lr: 0.0010\n",
      "Epoch 83/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2820\n",
      "Epoch 83: loss did not improve from 1.28030\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 1.2820 - lr: 0.0010\n",
      "Epoch 84/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2760\n",
      "Epoch 84: loss improved from 1.28030 to 1.27601, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.2760 - lr: 0.0010\n",
      "Epoch 85/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2678\n",
      "Epoch 85: loss improved from 1.27601 to 1.26777, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.2678 - lr: 0.0010\n",
      "Epoch 86/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2837\n",
      "Epoch 86: loss did not improve from 1.26777\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 1.2837 - lr: 0.0010\n",
      "Epoch 87/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2630\n",
      "Epoch 87: loss improved from 1.26777 to 1.26305, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.2630 - lr: 0.0010\n",
      "Epoch 88/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2477\n",
      "Epoch 88: loss improved from 1.26305 to 1.24770, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.2477 - lr: 0.0010\n",
      "Epoch 89/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2225\n",
      "Epoch 89: loss improved from 1.24770 to 1.22255, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.2225 - lr: 0.0010\n",
      "Epoch 90/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2229\n",
      "Epoch 90: loss did not improve from 1.22255\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.2229 - lr: 0.0010\n",
      "Epoch 91/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.2008\n",
      "Epoch 91: loss improved from 1.22255 to 1.20082, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.2008 - lr: 0.0010\n",
      "Epoch 92/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 1.1814\n",
      "Epoch 92: loss improved from 1.20082 to 1.18232, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.1823 - lr: 0.0010\n",
      "Epoch 93/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1729\n",
      "Epoch 93: loss improved from 1.18232 to 1.17290, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.1729 - lr: 0.0010\n",
      "Epoch 94/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1790\n",
      "Epoch 94: loss did not improve from 1.17290\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 1.1790 - lr: 0.0010\n",
      "Epoch 95/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1689\n",
      "Epoch 95: loss improved from 1.17290 to 1.16893, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 1.1689 - lr: 0.0010\n",
      "Epoch 96/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1480\n",
      "Epoch 96: loss improved from 1.16893 to 1.14797, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 1.1480 - lr: 0.0010\n",
      "Epoch 97/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1569\n",
      "Epoch 97: loss did not improve from 1.14797\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 1.1569 - lr: 0.0010\n",
      "Epoch 98/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1530\n",
      "Epoch 98: loss did not improve from 1.14797\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 1.1530 - lr: 0.0010\n",
      "Epoch 99/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 1.1520\n",
      "Epoch 99: loss did not improve from 1.14797\n",
      "\n",
      "Epoch 99: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 1.1520 - lr: 0.0010\n",
      "Epoch 100/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.8821\n",
      "Epoch 100: loss improved from 1.14797 to 0.88208, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.8821 - lr: 2.0000e-04\n",
      "Epoch 101/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7928\n",
      "Epoch 101: loss improved from 0.88208 to 0.79280, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7928 - lr: 2.0000e-04\n",
      "Epoch 102/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7669\n",
      "Epoch 102: loss improved from 0.79280 to 0.76693, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7669 - lr: 2.0000e-04\n",
      "Epoch 103/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.7506\n",
      "Epoch 103: loss improved from 0.76693 to 0.75253, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7525 - lr: 2.0000e-04\n",
      "Epoch 104/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7450\n",
      "Epoch 104: loss improved from 0.75253 to 0.74502, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7450 - lr: 2.0000e-04\n",
      "Epoch 105/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7416\n",
      "Epoch 105: loss improved from 0.74502 to 0.74158, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 0.7416 - lr: 2.0000e-04\n",
      "Epoch 106/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7374\n",
      "Epoch 106: loss improved from 0.74158 to 0.73738, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7374 - lr: 2.0000e-04\n",
      "Epoch 107/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7367\n",
      "Epoch 107: loss improved from 0.73738 to 0.73670, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7367 - lr: 2.0000e-04\n",
      "Epoch 108/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7348\n",
      "Epoch 108: loss improved from 0.73670 to 0.73476, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7348 - lr: 2.0000e-04\n",
      "Epoch 109/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.7318\n",
      "Epoch 109: loss improved from 0.73476 to 0.73239, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7324 - lr: 2.0000e-04\n",
      "Epoch 110/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7327\n",
      "Epoch 110: loss did not improve from 0.73239\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7327 - lr: 2.0000e-04\n",
      "Epoch 111/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7345\n",
      "Epoch 111: loss did not improve from 0.73239\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.7345 - lr: 2.0000e-04\n",
      "Epoch 112/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7319\n",
      "Epoch 112: loss improved from 0.73239 to 0.73194, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7319 - lr: 2.0000e-04\n",
      "Epoch 113/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7316\n",
      "Epoch 113: loss improved from 0.73194 to 0.73158, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7316 - lr: 2.0000e-04\n",
      "Epoch 114/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7320\n",
      "Epoch 114: loss did not improve from 0.73158\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.7320 - lr: 2.0000e-04\n",
      "Epoch 115/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7307\n",
      "Epoch 115: loss improved from 0.73158 to 0.73074, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 0.7307 - lr: 2.0000e-04\n",
      "Epoch 116/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7313\n",
      "Epoch 116: loss did not improve from 0.73074\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7313 - lr: 2.0000e-04\n",
      "Epoch 117/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7308\n",
      "Epoch 117: loss did not improve from 0.73074\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.7308 - lr: 2.0000e-04\n",
      "Epoch 118/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7291\n",
      "Epoch 118: loss improved from 0.73074 to 0.72914, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7291 - lr: 2.0000e-04\n",
      "Epoch 119/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7302\n",
      "Epoch 119: loss did not improve from 0.72914\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.7302 - lr: 2.0000e-04\n",
      "Epoch 120/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7298\n",
      "Epoch 120: loss did not improve from 0.72914\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.7298 - lr: 2.0000e-04\n",
      "Epoch 121/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7281\n",
      "Epoch 121: loss improved from 0.72914 to 0.72814, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7281 - lr: 2.0000e-04\n",
      "Epoch 122/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7272\n",
      "Epoch 122: loss improved from 0.72814 to 0.72725, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7272 - lr: 2.0000e-04\n",
      "Epoch 123/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7269\n",
      "Epoch 123: loss improved from 0.72725 to 0.72688, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7269 - lr: 2.0000e-04\n",
      "Epoch 124/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7283\n",
      "Epoch 124: loss did not improve from 0.72688\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.7283 - lr: 2.0000e-04\n",
      "Epoch 125/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7268\n",
      "Epoch 125: loss improved from 0.72688 to 0.72682, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7268 - lr: 2.0000e-04\n",
      "Epoch 126/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7263\n",
      "Epoch 126: loss improved from 0.72682 to 0.72629, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7263 - lr: 2.0000e-04\n",
      "Epoch 127/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7257\n",
      "Epoch 127: loss improved from 0.72629 to 0.72570, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7257 - lr: 2.0000e-04\n",
      "Epoch 128/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7254\n",
      "Epoch 128: loss improved from 0.72570 to 0.72545, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 129/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7252\n",
      "Epoch 129: loss improved from 0.72545 to 0.72524, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.7252 - lr: 2.0000e-04\n",
      "Epoch 130/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7247\n",
      "Epoch 130: loss improved from 0.72524 to 0.72465, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.7247 - lr: 2.0000e-04\n",
      "Epoch 131/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7254\n",
      "Epoch 131: loss did not improve from 0.72465\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.7254 - lr: 2.0000e-04\n",
      "Epoch 132/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7260\n",
      "Epoch 132: loss did not improve from 0.72465\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.7260 - lr: 2.0000e-04\n",
      "Epoch 133/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.7256\n",
      "Epoch 133: loss did not improve from 0.72465\n",
      "\n",
      "Epoch 133: ReduceLROnPlateau reducing learning rate to 0.0001.\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.7256 - lr: 2.0000e-04\n",
      "Epoch 134/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6786\n",
      "Epoch 134: loss improved from 0.72465 to 0.67857, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.6786 - lr: 1.0000e-04\n",
      "Epoch 135/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6765\n",
      "Epoch 135: loss improved from 0.67857 to 0.67649, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.6765 - lr: 1.0000e-04\n",
      "Epoch 136/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6761\n",
      "Epoch 136: loss improved from 0.67649 to 0.67613, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 81ms/step - loss: 0.6761 - lr: 1.0000e-04\n",
      "Epoch 137/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6757\n",
      "Epoch 137: loss improved from 0.67613 to 0.67566, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.6757 - lr: 1.0000e-04\n",
      "Epoch 138/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6748\n",
      "Epoch 138: loss improved from 0.67566 to 0.67479, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.6748 - lr: 1.0000e-04\n",
      "Epoch 139/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6744\n",
      "Epoch 139: loss improved from 0.67479 to 0.67441, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 80ms/step - loss: 0.6744 - lr: 1.0000e-04\n",
      "Epoch 140/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6745\n",
      "Epoch 140: loss did not improve from 0.67441\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.6745 - lr: 1.0000e-04\n",
      "Epoch 141/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6736\n",
      "Epoch 141: loss improved from 0.67441 to 0.67360, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.6736 - lr: 1.0000e-04\n",
      "Epoch 142/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6736\n",
      "Epoch 142: loss improved from 0.67360 to 0.67357, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.6736 - lr: 1.0000e-04\n",
      "Epoch 143/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6731\n",
      "Epoch 143: loss improved from 0.67357 to 0.67307, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.6731 - lr: 1.0000e-04\n",
      "Epoch 144/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6737\n",
      "Epoch 144: loss did not improve from 0.67307\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.6737 - lr: 1.0000e-04\n",
      "Epoch 145/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6739\n",
      "Epoch 145: loss did not improve from 0.67307\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.6739 - lr: 1.0000e-04\n",
      "Epoch 146/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6733\n",
      "Epoch 146: loss did not improve from 0.67307\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.6733 - lr: 1.0000e-04\n",
      "Epoch 147/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6743\n",
      "Epoch 147: loss did not improve from 0.67307\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.6743 - lr: 1.0000e-04\n",
      "Epoch 148/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6725\n",
      "Epoch 148: loss improved from 0.67307 to 0.67254, saving model to nextword1.h5\n",
      "74/74 [==============================] - 6s 79ms/step - loss: 0.6725 - lr: 1.0000e-04\n",
      "Epoch 149/150\n",
      "73/74 [============================>.] - ETA: 0s - loss: 0.6724\n",
      "Epoch 149: loss did not improve from 0.67254\n",
      "74/74 [==============================] - 6s 77ms/step - loss: 0.6729 - lr: 1.0000e-04\n",
      "Epoch 150/150\n",
      "74/74 [==============================] - ETA: 0s - loss: 0.6736\n",
      "Epoch 150: loss did not improve from 0.67254\n",
      "74/74 [==============================] - 6s 78ms/step - loss: 0.6736 - lr: 1.0000e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c5a245d460>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))\n",
    "model.fit(X, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "077290c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = load_model(\"C:/Users/Mohammed Qadir/Untitled Folder/nextword1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3d36e3a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 1, 10)             30540     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 1, 1000)           4044000   \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 1000)              8004000   \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1000)              1001000   \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 3054)              3057054   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 16,136,594\n",
      "Trainable params: 16,136,594\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "101d6258",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# read the test file\n",
    "with open(\"E:/Internshp/test.txt\", 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# preprocess the text\n",
    "text = text.replace('\\n', ' ').replace('\\r', '').strip()\n",
    "\n",
    "# tokenize the text\n",
    "encoded_text = tokenizer.encode(text, add_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d28d5e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 11, but `max_length` is set to 3. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For writers, a random sentence can help them get their message\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# set the device to GPU if available, otherwise CPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load the trained model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.to(device)\n",
    "\n",
    "# set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# convert the encoded text to a PyTorch tensor\n",
    "input_ids = torch.tensor(encoded_text).unsqueeze(0).to(device)\n",
    "\n",
    "# generate text\n",
    "generated_text = model.generate(input_ids, do_sample=True, max_length=3, top_k=50, top_p=0.95)\n",
    "decoded_text = tokenizer.decode(generated_text.squeeze().tolist())\n",
    "\n",
    "print(decoded_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8be6e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
